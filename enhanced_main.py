
# Enhanced AI Backend Service - All Medical Analysis Algorithms
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.cluster import DBSCAN, KMeans
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, LSTM, Input, Dropout, BatchNormalization
from tensorflow.keras.layers import Embedding, MultiHeadAttention, LayerNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from transformers import AutoTokenizer, AutoModel, pipeline
import cv2
from flask import Flask, request, jsonify
from flask_cors import CORS
import joblib
import os
import base64
from PIL import Image
import io
import warnings
import scipy.stats as stats
from scipy import signal
import pickle
import requests
from datetime import datetime, timedelta
warnings.filterwarnings('ignore')

app = Flask(__name__)
CORS(app)

class EnhancedMedicalAIAnalyzer:
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.medical_thresholds = {
            'glucose': {'normal': (70, 140), 'prediabetes': (140, 200), 'diabetes': (200, float('inf'))},
            'cholesterol': {'normal': (0, 200), 'borderline': (200, 240), 'high': (240, float('inf'))},
            'blood_pressure_systolic': {'normal': (90, 120), 'elevated': (120, 130), 'stage1': (130, 140), 'stage2': (140, float('inf'))},
            'hemoglobin': {'low': (0, 12), 'normal': (12, 16), 'high': (16, float('inf'))},
            'heart_rate': {'low': (0, 60), 'normal': (60, 100), 'high': (100, float('inf'))}
        }
        self.load_or_train_models()
    
    def load_or_train_models(self):
        """ØªØ­Ù…ÙŠÙ„ Ø£Ùˆ ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†Ø©"""
        print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ­Ù…ÙŠÙ„ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø·Ø¨ÙŠ...")
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ© Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø©
        self.prepare_medical_datasets()
        
        # 1. Logistic Regression Ø§Ù„Ù…Ø­Ø³Ù†
        self.models['logistic'] = LogisticRegression(
            solver='liblinear', 
            random_state=42, 
            class_weight='balanced'
        )
        
        # 2. Random Forest Ø§Ù„Ù…Ø­Ø³Ù†
        self.models['random_forest'] = RandomForestClassifier(
            n_estimators=200, 
            max_depth=10, 
            random_state=42,
            class_weight='balanced'
        )
        
        # 3. SVM Ø§Ù„Ù…Ø­Ø³Ù†
        self.models['svm'] = SVC(
            kernel='rbf', 
            probability=True, 
            random_state=42,
            class_weight='balanced',
            gamma='scale'
        )
        
        # 4. KNN Ø§Ù„Ù…Ø­Ø³Ù†
        self.models['knn'] = KNeighborsClassifier(
            n_neighbors=7, 
            weights='distance',
            metric='manhattan'
        )
        
        # 5. Naive Bayes Ø§Ù„Ù…Ø­Ø³Ù†
        self.models['naive_bayes'] = GaussianNB(var_smoothing=1e-9)
        
        # 6. Decision Tree Ø§Ù„Ù…Ø­Ø³Ù†
        self.models['decision_tree'] = DecisionTreeClassifier(
            random_state=42,
            max_depth=15,
            min_samples_split=5,
            class_weight='balanced'
        )
        
        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        self.train_basic_models()
        
        # 7-10. Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        self.build_advanced_neural_networks()
        
        print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø¨Ù†Ø¬Ø§Ø­")
    
    def prepare_medical_datasets(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ© Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø©"""
        np.random.seed(42)
        
        # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ø§Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        self.basic_features = 15  # Ø¹Ø¯Ø¯ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø·Ø¨ÙŠØ©
        self.n_samples = 5000
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø·Ø¨ÙŠØ© ÙˆØ§Ù‚Ø¹ÙŠØ©
        self.X_medical = self.generate_realistic_medical_data()
        self.y_medical = self.generate_medical_labels()
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            self.X_medical, self.y_medical, test_size=0.2, random_state=42, stratify=self.y_medical
        )
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self.scaler = StandardScaler()
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)
    
    def generate_realistic_medical_data(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø·Ø¨ÙŠØ© ÙˆØ§Ù‚Ø¹ÙŠØ©"""
        data = np.random.randn(self.n_samples, self.basic_features)
        
        # Ù…Ø­Ø§ÙƒØ§Ø© Ù‚ÙŠÙ… Ø·Ø¨ÙŠØ© ÙˆØ§Ù‚Ø¹ÙŠØ©
        # Ø§Ù„Ø³ÙƒØ± (glucose)
        data[:, 0] = np.random.normal(120, 30, self.n_samples)
        # Ø§Ù„ÙƒÙˆÙ„ÙŠØ³ØªØ±ÙˆÙ„
        data[:, 1] = np.random.normal(180, 40, self.n_samples)
        # Ø¶ØºØ· Ø§Ù„Ø¯Ù… Ø§Ù„Ø§Ù†Ù‚Ø¨Ø§Ø¶ÙŠ
        data[:, 2] = np.random.normal(125, 20, self.n_samples)
        # Ø¶ØºØ· Ø§Ù„Ø¯Ù… Ø§Ù„Ø§Ù†Ø¨Ø³Ø§Ø·ÙŠ
        data[:, 3] = np.random.normal(80, 15, self.n_samples)
        # Ù…Ø¹Ø¯Ù„ Ø¶Ø±Ø¨Ø§Øª Ø§Ù„Ù‚Ù„Ø¨
        data[:, 4] = np.random.normal(75, 12, self.n_samples)
        # Ø§Ù„Ù‡ÙŠÙ…ÙˆØ¬Ù„ÙˆØ¨ÙŠÙ†
        data[:, 5] = np.random.normal(14, 2, self.n_samples)
        # Ø®Ù„Ø§ÙŠØ§ Ø§Ù„Ø¯Ù… Ø§Ù„Ø¨ÙŠØ¶Ø§Ø¡
        data[:, 6] = np.random.normal(7000, 2000, self.n_samples)
        # Ø§Ù„ØµÙØ§Ø¦Ø­ Ø§Ù„Ø¯Ù…ÙˆÙŠØ©
        data[:, 7] = np.random.normal(300000, 50000, self.n_samples)
        # Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø©
        data[:, 8] = np.random.normal(37, 0.8, self.n_samples)
        # Ø§Ù„ØªØ´Ø¨Ø¹ Ø¨Ø§Ù„Ø£ÙƒØ³Ø¬ÙŠÙ†
        data[:, 9] = np.random.normal(98, 2, self.n_samples)
        
        return data
    
    def generate_medical_labels(self):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªØµÙ†ÙŠÙØ§Øª Ø·Ø¨ÙŠØ©"""
        labels = np.zeros(self.n_samples)
        
        # Ù…Ù†Ø·Ù‚ ØªØµÙ†ÙŠÙ Ù…Ø¹Ù‚Ø¯ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø·Ø¨ÙŠØ©
        for i in range(self.n_samples):
            risk_score = 0
            
            # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ø®Ø·Ø±
            if self.X_medical[i, 0] > 140:  # Ø³ÙƒØ± Ù…Ø±ØªÙØ¹
                risk_score += 2
            if self.X_medical[i, 1] > 240:  # ÙƒÙˆÙ„ÙŠØ³ØªØ±ÙˆÙ„ Ù…Ø±ØªÙØ¹
                risk_score += 2
            if self.X_medical[i, 2] > 140:  # Ø¶ØºØ· Ø¯Ù… Ù…Ø±ØªÙØ¹
                risk_score += 2
            if self.X_medical[i, 4] > 100 or self.X_medical[i, 4] < 60:  # Ù…Ø¹Ø¯Ù„ Ù‚Ù„Ø¨ ØºÙŠØ± Ø·Ø¨ÙŠØ¹ÙŠ
                risk_score += 1
            
            # ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø®Ø§Ø·Ø±
            if risk_score >= 4:
                labels[i] = 3  # Ø®Ø·Ø± Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹
            elif risk_score >= 3:
                labels[i] = 2  # Ø®Ø·Ø± Ø¹Ø§Ù„ÙŠ
            elif risk_score >= 1:
                labels[i] = 1  # Ø®Ø·Ø± Ù…ØªÙˆØ³Ø·
            else:
                labels[i] = 0  # Ø·Ø¨ÙŠØ¹ÙŠ
        
        return labels.astype(int)
    
    def train_basic_models(self):
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"""
        print("ğŸ“š ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©...")
        
        for name, model in self.models.items():
            if name in ['logistic', 'random_forest', 'svm', 'knn', 'naive_bayes', 'decision_tree']:
                try:
                    model.fit(self.X_train_scaled, self.y_train)
                    accuracy = model.score(self.X_test_scaled, self.y_test)
                    print(f"âœ… Ù†Ù…ÙˆØ°Ø¬ {name}: Ø¯Ù‚Ø© {accuracy:.3f}")
                except Exception as e:
                    print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ {name}: {e}")
    
    def build_advanced_neural_networks(self):
        """Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        print("ğŸ§  Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©...")
        
        # 7. Enhanced Neural Network
        self.models['neural_network'] = Sequential([
            Dense(256, activation='relu', input_shape=(self.basic_features,)),
            BatchNormalization(),
            Dropout(0.3),
            Dense(128, activation='relu'),
            BatchNormalization(),
            Dropout(0.2),
            Dense(64, activation='relu'),
            Dropout(0.1),
            Dense(4, activation='softmax')  # 4 ÙØ¦Ø§Øª Ù„Ù„Ù…Ø®Ø§Ø·Ø±
        ])
        self.models['neural_network'].compile(
            optimizer='adam', 
            loss='sparse_categorical_crossentropy', 
            metrics=['accuracy']
        )
        
        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©
        try:
            self.models['neural_network'].fit(
                self.X_train_scaled, self.y_train,
                epochs=50, batch_size=32, verbose=0,
                validation_data=(self.X_test_scaled, self.y_test)
            )
            print("âœ… Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©: ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨")
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©: {e}")
        
        # 8. CNN Ù„Ù„ØµÙˆØ± Ø§Ù„Ø·Ø¨ÙŠØ©
        self.models['cnn'] = Sequential([
            Conv2D(64, (3, 3), activation='relu', input_shape=(224, 224, 3)),
            BatchNormalization(),
            MaxPooling2D((2, 2)),
            Conv2D(128, (3, 3), activation='relu'),
            BatchNormalization(),
            MaxPooling2D((2, 2)),
            Conv2D(256, (3, 3), activation='relu'),
            BatchNormalization(),
            MaxPooling2D((2, 2)),
            Flatten(),
            Dense(512, activation='relu'),
            Dropout(0.5),
            Dense(256, activation='relu'),
            Dropout(0.3),
            Dense(4, activation='softmax')
        ])
        self.models['cnn'].compile(
            optimizer='adam', 
            loss='sparse_categorical_crossentropy', 
            metrics=['accuracy']
        )
        print("âœ… CNN: ØªÙ… Ø§Ù„Ø¨Ù†Ø§Ø¡")
        
        # 9. LSTM Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ©
        self.models['lstm'] = Sequential([
            LSTM(128, return_sequences=True, input_shape=(30, 1)),
            Dropout(0.2),
            LSTM(64, return_sequences=True),
            Dropout(0.2),
            LSTM(32, return_sequences=False),
            Dense(64, activation='relu'),
            Dropout(0.1),
            Dense(4, activation='softmax')
        ])
        self.models['lstm'].compile(
            optimizer='adam', 
            loss='sparse_categorical_crossentropy', 
            metrics=['accuracy']
        )
        print("âœ… LSTM: ØªÙ… Ø§Ù„Ø¨Ù†Ø§Ø¡")
        
        # 10. Transformer Ù„Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
        self.build_transformer_model()
    
    def build_transformer_model(self):
        """Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Transformer Ù…Ø¨Ø³Ø·"""
        try:
            inputs = Input(shape=(self.basic_features,))
            
            # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØªØ³Ù„Ø³Ù„
            reshaped = tf.expand_dims(inputs, axis=1)
            
            # Ø·Ø¨Ù‚Ø© Ø§Ù†ØªØ¨Ø§Ù‡ Ù…Ø¨Ø³Ø·Ø©
            attention = MultiHeadAttention(num_heads=4, key_dim=32)(reshaped, reshaped)
            attention = LayerNormalization()(attention)
            
            # ØªØ³Ø·ÙŠØ­ Ø§Ù„Ù†ØªÙŠØ¬Ø©
            flattened = Flatten()(attention)
            
            # Ø·Ø¨Ù‚Ø§Øª ÙƒØ«ÙŠÙØ©
            dense1 = Dense(128, activation='relu')(flattened)
            dropout1 = Dropout(0.2)(dense1)
            dense2 = Dense(64, activation='relu')(dropout1)
            outputs = Dense(4, activation='softmax')(dense2)
            
            self.models['transformer'] = Model(inputs=inputs, outputs=outputs)
            self.models['transformer'].compile(
                optimizer='adam',
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy']
            )
            print("âœ… Transformer: ØªÙ… Ø§Ù„Ø¨Ù†Ø§Ø¡")
        except Exception as e:
            print(f"âš ï¸ Transformer ØºÙŠØ± Ù…ØªÙˆÙØ±: {e}")
    
    def advanced_medical_analysis(self, lab_data, algorithm='ensemble'):
        """ØªØ­Ù„ÙŠÙ„ Ø·Ø¨ÙŠ Ù…ØªÙ‚Ø¯Ù… Ø´Ø§Ù…Ù„"""
        try:
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if isinstance(lab_data, dict):
                features = self.extract_features_from_dict(lab_data)
            else:
                features = np.array(lab_data)
            
            if features.shape[0] < self.basic_features:
                # Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø§Ù‚ØµØ© Ø¨Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
                features = self.pad_features(features)
            
            features = features.reshape(1, -1)
            features_scaled = self.scaler.transform(features)
            
            # Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„
            result = {
                'primary_analysis': {},
                'risk_assessment': {},
                'medical_insights': {},
                'recommendations': {},
                'confidence_metrics': {}
            }
            
            if algorithm == 'ensemble':
                result = self.ensemble_analysis(features_scaled, features)
            else:
                result = self.single_algorithm_analysis(features_scaled, algorithm, features)
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
            result['advanced_metrics'] = self.calculate_advanced_metrics(features[0])
            result['trend_analysis'] = self.analyze_trends(features[0])
            result['anomaly_detection'] = self.detect_medical_anomalies(features[0])
            
            return result
            
        except Exception as e:
            return {'error': f'Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…: {str(e)}'}
    
    def extract_features_from_dict(self, lab_data):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ù…Ù† Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        feature_order = [
            'glucose', 'cholesterol', 'bloodPressureSystolic', 'bloodPressureDiastolic',
            'heartRate', 'hemoglobin', 'whiteBloodCells', 'platelets',
            'temperature', 'oxygenSaturation'
        ]
        
        features = []
        for feature in feature_order:
            value = lab_data.get(feature, 0)
            features.append(float(value) if value else 0)
        
        # Ø¥Ø¶Ø§ÙØ© Ø®ØµØ§Ø¦Øµ Ù…Ø´ØªÙ‚Ø©
        if len(features) >= 4:
            # Ø¶ØºØ· Ø§Ù„Ù†Ø¨Ø¶ (Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø§Ù†Ù‚Ø¨Ø§Ø¶ÙŠ ÙˆØ§Ù„Ø§Ù†Ø¨Ø³Ø§Ø·ÙŠ)
            pulse_pressure = features[2] - features[3] if features[2] and features[3] else 0
            features.append(pulse_pressure)
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¤Ø´Ø±Ø§Øª Ø£Ø®Ø±Ù‰ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©
        while len(features) < self.basic_features:
            features.append(0)
        
        return np.array(features[:self.basic_features])
    
    def pad_features(self, features):
        """Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ø§Ù‚ØµØ©"""
        if len(features) < self.basic_features:
            padding = np.zeros(self.basic_features - len(features))
            features = np.concatenate([features, padding])
        return features[:self.basic_features]
    
    def ensemble_analysis(self, features_scaled, original_features):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙØ±Ù‚Ø© Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚ÙŠØ© (Ensemble)"""
        predictions = {}
        probabilities = {}
        
        # Ø¬Ù…Ø¹ ØªÙ†Ø¨Ø¤Ø§Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        for name, model in self.models.items():
            if name in ['logistic', 'random_forest', 'svm', 'knn', 'naive_bayes', 'decision_tree']:
                try:
                    if hasattr(model, 'predict_proba'):
                        prob = model.predict_proba(features_scaled)[0]
                        probabilities[name] = prob.tolist()
                        predictions[name] = np.argmax(prob)
                    else:
                        pred = model.predict(features_scaled)[0]
                        predictions[name] = pred
                except Exception as e:
                    print(f"Ø®Ø·Ø£ ÙÙŠ {name}: {e}")
        
        # ØªÙ†Ø¨Ø¤ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©
        try:
            nn_prob = self.models['neural_network'].predict(features_scaled, verbose=0)[0]
            probabilities['neural_network'] = nn_prob.tolist()
            predictions['neural_network'] = np.argmax(nn_prob)
        except:
            pass
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        if predictions:
            ensemble_prediction = np.mean(list(predictions.values()))
            risk_level = self.determine_risk_level(ensemble_prediction)
            
            return {
                'ensemble_prediction': float(ensemble_prediction),
                'individual_predictions': predictions,
                'probabilities': probabilities,
                'risk_level': risk_level,
                'confidence': self.calculate_ensemble_confidence(probabilities),
                'medical_interpretation': self.interpret_medical_results(original_features[0], risk_level)
            }
        
        return {'error': 'Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙ†Ø¨Ø¤Ø§Øª Ù…ØªØ§Ø­Ø©'}
    
    def single_algorithm_analysis(self, features_scaled, algorithm, original_features):
        """ØªØ­Ù„ÙŠÙ„ Ø¨Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© ÙˆØ§Ø­Ø¯Ø©"""
        model = self.models.get(algorithm)
        if not model:
            return {'error': f'Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© {algorithm} ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©'}
        
        try:
            if algorithm == 'neural_network':
                prob = model.predict(features_scaled, verbose=0)[0]
                prediction = np.argmax(prob)
                confidence = np.max(prob)
            elif hasattr(model, 'predict_proba'):
                prob = model.predict_proba(features_scaled)[0]
                prediction = np.argmax(prob)
                confidence = np.max(prob)
            else:
                prediction = model.predict(features_scaled)[0]
                confidence = 0.8  # Ø«Ù‚Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
            
            risk_level = self.determine_risk_level(prediction)
            
            return {
                'prediction': float(prediction),
                'algorithm': algorithm,
                'confidence': float(confidence),
                'risk_level': risk_level,
                'medical_interpretation': self.interpret_medical_results(original_features[0], risk_level)
            }
        except Exception as e:
            return {'error': f'Ø®Ø·Ø£ ÙÙŠ {algorithm}: {str(e)}'}
    
    def determine_risk_level(self, prediction):
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø®Ø§Ø·Ø±"""
        if prediction >= 3:
            return 'critical'
        elif prediction >= 2:
            return 'high'
        elif prediction >= 1:
            return 'medium'
        else:
            return 'low'
    
    def interpret_medical_results(self, features, risk_level):
        """ØªÙØ³ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø·Ø¨ÙŠØ©"""
        interpretation = {
            'glucose_status': self.interpret_glucose(features[0]),
            'cholesterol_status': self.interpret_cholesterol(features[1]),
            'blood_pressure_status': self.interpret_blood_pressure(features[2], features[3]),
            'heart_rate_status': self.interpret_heart_rate(features[4]),
            'overall_assessment': self.get_overall_assessment(risk_level)
        }
        return interpretation
    
    def interpret_glucose(self, glucose):
        """ØªÙØ³ÙŠØ± Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø³ÙƒØ±"""
        if glucose < 70:
            return {'status': 'Ù…Ù†Ø®ÙØ¶', 'recommendation': 'ÙŠÙÙ†ØµØ­ Ø¨ØªÙ†Ø§ÙˆÙ„ Ø´ÙŠØ¡ Ø­Ù„Ùˆ ÙÙˆØ±Ø§Ù‹'}
        elif glucose <= 140:
            return {'status': 'Ø·Ø¨ÙŠØ¹ÙŠ', 'recommendation': 'Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø³ÙƒØ± ÙÙŠ Ø§Ù„Ù…Ø¯Ù‰ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ'}
        elif glucose <= 200:
            return {'status': 'Ù…Ù‚Ø¯Ù…Ø§Øª Ø§Ù„Ø³ÙƒØ±ÙŠ', 'recommendation': 'ÙŠÙÙ†ØµØ­ Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø·Ø¨ÙŠØ¨ ÙˆØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØºØ°Ø§Ø¦ÙŠ'}
        else:
            return {'status': 'Ø³ÙƒØ±ÙŠ', 'recommendation': 'Ù…Ø±Ø§Ø¬Ø¹Ø© Ø·Ø¨ÙŠØ© Ø¹Ø§Ø¬Ù„Ø© Ù…Ø·Ù„ÙˆØ¨Ø©'}
    
    def interpret_cholesterol(self, cholesterol):
        """ØªÙØ³ÙŠØ± Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙƒÙˆÙ„ÙŠØ³ØªØ±ÙˆÙ„"""
        if cholesterol < 200:
            return {'status': 'Ø·Ø¨ÙŠØ¹ÙŠ', 'recommendation': 'Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙƒÙˆÙ„ÙŠØ³ØªØ±ÙˆÙ„ Ø¬ÙŠØ¯'}
        elif cholesterol < 240:
            return {'status': 'Ø­Ø¯ÙŠ', 'recommendation': 'ÙŠÙÙ†ØµØ­ Ø¨ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù‡ÙˆÙ† ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØºØ°Ø§Ø¦ÙŠ'}
        else:
            return {'status': 'Ù…Ø±ØªÙØ¹', 'recommendation': 'Ù…Ø±Ø§Ø¬Ø¹Ø© Ø·Ø¨ÙŠØ© Ù„ÙˆØµÙ Ø§Ù„Ø¹Ù„Ø§Ø¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨'}
    
    def interpret_blood_pressure(self, systolic, diastolic):
        """ØªÙØ³ÙŠØ± Ø¶ØºØ· Ø§Ù„Ø¯Ù…"""
        if systolic < 90 or diastolic < 60:
            return {'status': 'Ù…Ù†Ø®ÙØ¶', 'recommendation': 'Ù…Ø±Ø§Ø¬Ø¹Ø© Ø·Ø¨ÙŠØ© Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø³Ø¨Ø¨'}
        elif systolic <= 120 and diastolic <= 80:
            return {'status': 'Ø·Ø¨ÙŠØ¹ÙŠ', 'recommendation': 'Ø¶ØºØ· Ø§Ù„Ø¯Ù… ÙÙŠ Ø§Ù„Ù…Ø¯Ù‰ Ø§Ù„Ù…Ø«Ø§Ù„ÙŠ'}
        elif systolic <= 140 or diastolic <= 90:
            return {'status': 'Ù…Ø±ØªÙØ¹ Ù‚Ù„ÙŠÙ„Ø§Ù‹', 'recommendation': 'Ù…Ø±Ø§Ù‚Ø¨Ø© Ø¯ÙˆØ±ÙŠØ© ÙˆØªØ¹Ø¯ÙŠÙ„ Ù†Ù…Ø· Ø§Ù„Ø­ÙŠØ§Ø©'}
        else:
            return {'status': 'Ù…Ø±ØªÙØ¹', 'recommendation': 'Ù…Ø±Ø§Ø¬Ø¹Ø© Ø·Ø¨ÙŠØ© Ø¹Ø§Ø¬Ù„Ø© Ù„ÙˆØµÙ Ø§Ù„Ø¹Ù„Ø§Ø¬'}
    
    def interpret_heart_rate(self, heart_rate):
        """ØªÙØ³ÙŠØ± Ù…Ø¹Ø¯Ù„ Ø¶Ø±Ø¨Ø§Øª Ø§Ù„Ù‚Ù„Ø¨"""
        if heart_rate < 60:
            return {'status': 'Ø¨Ø·Ø¡', 'recommendation': 'Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø·Ø¨ÙŠØ¹ÙŠØ§Ù‹ Ù„Ù„Ø±ÙŠØ§Ø¶ÙŠÙŠÙ†ØŒ ÙˆØ¥Ù„Ø§ ÙÙŠÙÙ†ØµØ­ Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© Ø·Ø¨ÙŠØ©'}
        elif heart_rate <= 100:
            return {'status': 'Ø·Ø¨ÙŠØ¹ÙŠ', 'recommendation': 'Ù…Ø¹Ø¯Ù„ Ø¶Ø±Ø¨Ø§Øª Ø§Ù„Ù‚Ù„Ø¨ ÙÙŠ Ø§Ù„Ù…Ø¯Ù‰ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ'}
        else:
            return {'status': 'Ø³Ø±ÙŠØ¹', 'recommendation': 'ÙŠÙÙ†ØµØ­ Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© Ø·Ø¨ÙŠØ© Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø³Ø¨Ø¨'}
    
    def get_overall_assessment(self, risk_level):
        """Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø´Ø§Ù…Ù„"""
        assessments = {
            'low': {'status': 'Ø­Ø§Ù„Ø© ØµØ­ÙŠØ© Ø¬ÙŠØ¯Ø©', 'action': 'Ù…ØªØ§Ø¨Ø¹Ø© Ø¯ÙˆØ±ÙŠØ©'},
            'medium': {'status': 'ÙŠØ­ØªØ§Ø¬ Ù…ØªØ§Ø¨Ø¹Ø©', 'action': 'Ù…Ø±Ø§Ø¬Ø¹Ø© Ø·Ø¨ÙŠØ© Ø®Ù„Ø§Ù„ Ø£Ø³Ø¨ÙˆØ¹'},
            'high': {'status': 'ÙŠØ­ØªØ§Ø¬ ØªØ¯Ø®Ù„ Ø·Ø¨ÙŠ', 'action': 'Ù…Ø±Ø§Ø¬Ø¹Ø© Ø·Ø¨ÙŠØ© Ø®Ù„Ø§Ù„ ÙŠÙˆÙ…ÙŠÙ†'},
            'critical': {'status': 'Ø­Ø§Ù„Ø© Ø·Ø§Ø±Ø¦Ø©', 'action': 'Ù…Ø±Ø§Ø¬Ø¹Ø© Ø·Ø¨ÙŠØ© ÙÙˆØ±ÙŠØ©'}
        }
        return assessments.get(risk_level, assessments['medium'])
    
    def calculate_advanced_metrics(self, features):
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        return {
            'metabolic_syndrome_risk': self.calculate_metabolic_syndrome_risk(features),
            'cardiovascular_risk': self.calculate_cardiovascular_risk(features),
            'diabetes_risk': self.calculate_diabetes_risk(features),
            'overall_health_score': self.calculate_health_score(features)
        }
    
    def calculate_metabolic_syndrome_risk(self, features):
        """Ø­Ø³Ø§Ø¨ Ù…Ø®Ø§Ø·Ø± Ù…ØªÙ„Ø§Ø²Ù…Ø© Ø§Ù„Ø£ÙŠØ¶"""
        risk_factors = 0
        
        # Ù…Ø¹Ø§ÙŠÙŠØ± Ù…ØªÙ„Ø§Ø²Ù…Ø© Ø§Ù„Ø£ÙŠØ¶
        if features[0] >= 100:  # Ø³ÙƒØ± ØµØ§Ø¦Ù… â‰¥ 100
            risk_factors += 1
        if features[2] >= 130 or features[3] >= 85:  # Ø¶ØºØ· Ø¯Ù… â‰¥ 130/85
            risk_factors += 1
        if features[1] >= 200:  # ÙƒÙˆÙ„ÙŠØ³ØªØ±ÙˆÙ„ Ù…Ø±ØªÙØ¹
            risk_factors += 1
        
        risk_percentage = (risk_factors / 3) * 100
        return {
            'risk_factors_count': risk_factors,
            'risk_percentage': risk_percentage,
            'classification': 'Ø¹Ø§Ù„ÙŠ' if risk_factors >= 2 else 'Ù…ØªÙˆØ³Ø·' if risk_factors == 1 else 'Ù…Ù†Ø®ÙØ¶'
        }
    
    def calculate_cardiovascular_risk(self, features):
        """Ø­Ø³Ø§Ø¨ Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù‚Ù„Ø¨ ÙˆØ§Ù„Ø£ÙˆØ¹ÙŠØ© Ø§Ù„Ø¯Ù…ÙˆÙŠØ©"""
        # Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¨Ø³Ø· Ù„ØªÙ‚ÙŠÙŠÙ… Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù‚Ù„Ø¨
        risk_score = 0
        
        if features[2] > 140:  # Ø¶ØºØ· Ø¯Ù… Ù…Ø±ØªÙØ¹
            risk_score += 2
        if features[1] > 240:  # ÙƒÙˆÙ„ÙŠØ³ØªØ±ÙˆÙ„ Ù…Ø±ØªÙØ¹
            risk_score += 2
        if features[0] > 126:  # Ø³ÙƒØ±ÙŠ
            risk_score += 2
        if features[4] > 100 or features[4] < 60:  # Ù…Ø¹Ø¯Ù„ Ù‚Ù„Ø¨ ØºÙŠØ± Ø·Ø¨ÙŠØ¹ÙŠ
            risk_score += 1
        
        risk_percentage = min((risk_score / 7) * 100, 100)
        return {
            'risk_score': risk_score,
            'risk_percentage': risk_percentage,
            'classification': 'Ø¹Ø§Ù„ÙŠ' if risk_score >= 4 else 'Ù…ØªÙˆØ³Ø·' if risk_score >= 2 else 'Ù…Ù†Ø®ÙØ¶'
        }
    
    def calculate_diabetes_risk(self, features):
        """Ø­Ø³Ø§Ø¨ Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ø³ÙƒØ±ÙŠ"""
        if features[0] >= 126:
            return {'risk': 'Ù…Ø¤ÙƒØ¯', 'percentage': 95}
        elif features[0] >= 100:
            return {'risk': 'Ø¹Ø§Ù„ÙŠ', 'percentage': 70}
        elif features[0] >= 90:
            return {'risk': 'Ù…ØªÙˆØ³Ø·', 'percentage': 30}
        else:
            return {'risk': 'Ù…Ù†Ø®ÙØ¶', 'percentage': 10}
    
    def calculate_health_score(self, features):
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„ØµØ­ÙŠØ© Ø§Ù„Ø´Ø§Ù…Ù„Ø©"""
        total_score = 100
        
        # Ø®ØµÙ… Ù†Ù‚Ø§Ø· Ù„Ù„Ù‚ÙŠÙ… ØºÙŠØ± Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©
        if features[0] > 140 or features[0] < 70:  # Ø³ÙƒØ±
            total_score -= 20
        if features[1] > 240:  # ÙƒÙˆÙ„ÙŠØ³ØªØ±ÙˆÙ„
            total_score -= 15
        if features[2] > 140 or features[2] < 90:  # Ø¶ØºØ· Ø¯Ù…
            total_score -= 20
        if features[4] > 100 or features[4] < 60:  # Ù…Ø¹Ø¯Ù„ Ù‚Ù„Ø¨
            total_score -= 10
        if features[8] > 38 or features[8] < 36:  # Ø­Ø±Ø§Ø±Ø©
            total_score -= 5
        
        return max(total_score, 0)
    
    def analyze_trends(self, features):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª"""
        # Ù…Ø­Ø§ÙƒØ§Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª (ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ø³ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©)
        return {
            'glucose_trend': 'Ù…Ø³ØªÙ‚Ø±',
            'blood_pressure_trend': 'Ù…ØªØ²Ø§ÙŠØ¯ Ù‚Ù„ÙŠÙ„Ø§Ù‹',
            'heart_rate_trend': 'Ù…Ø³ØªÙ‚Ø±',
            'overall_trend': 'ØªØ­Ø³Ù† Ø·ÙÙŠÙ'
        }
    
    def detect_medical_anomalies(self, features):
        """ÙƒØ´Ù Ø§Ù„Ø´Ø°ÙˆØ° Ø§Ù„Ø·Ø¨ÙŠ"""
        anomalies = []
        
        # ÙØ­Øµ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø©
        if features[0] > 300:  # Ø³ÙƒØ± Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹
            anomalies.append({'type': 'Ø³ÙƒØ± Ù…Ø±ØªÙØ¹ Ø¨Ø´Ø¯Ø©', 'severity': 'critical'})
        if features[2] > 180:  # Ø¶ØºØ· Ø¯Ù… Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹
            anomalies.append({'type': 'Ø§Ø±ØªÙØ§Ø¹ Ø´Ø¯ÙŠØ¯ ÙÙŠ Ø¶ØºØ· Ø§Ù„Ø¯Ù…', 'severity': 'critical'})
        if features[4] > 150:  # Ù†Ø¨Ø¶ Ø³Ø±ÙŠØ¹ Ø¬Ø¯Ø§Ù‹
            anomalies.append({'type': 'ØªØ³Ø§Ø±Ø¹ Ø´Ø¯ÙŠØ¯ ÙÙŠ Ø§Ù„Ù‚Ù„Ø¨', 'severity': 'high'})
        
        return {
            'anomalies_detected': len(anomalies) > 0,
            'anomalies': anomalies,
            'severity_level': 'critical' if any(a['severity'] == 'critical' for a in anomalies) else 'high' if anomalies else 'normal'
        }
    
    def calculate_ensemble_confidence(self, probabilities):
        """Ø­Ø³Ø§Ø¨ Ø«Ù‚Ø© Ø§Ù„ÙØ±Ù‚Ø© Ø§Ù„Ù…ÙˆØ³ÙŠÙ‚ÙŠØ©"""
        if not probabilities:
            return 0.5
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ Ù„Ù„ØªÙ†Ø¨Ø¤Ø§Øª
        all_predictions = []
        for model_probs in probabilities.values():
            if isinstance(model_probs, list):
                all_predictions.append(np.argmax(model_probs))
        
        if len(all_predictions) < 2:
            return 0.8
        
        std_dev = np.std(all_predictions)
        confidence = max(0.1, 1.0 - (std_dev / 2.0))
        return float(confidence)

# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø«ÙŠÙ„ Ø§Ù„Ù…Ø­Ù„Ù„
analyzer = EnhancedMedicalAIAnalyzer()

# API Endpoints
@app.route('/api/analyze/comprehensive', methods=['POST'])
def analyze_comprehensive():
    """ØªØ­Ù„ÙŠÙ„ Ø·Ø¨ÙŠ Ø´Ø§Ù…Ù„ Ù…ØªÙ‚Ø¯Ù…"""
    try:
        data = request.json
        lab_data = data.get('lab_values', {})
        algorithm = data.get('algorithm', 'ensemble')
        
        result = analyzer.advanced_medical_analysis(lab_data, algorithm)
        return jsonify({
            'success': True,
            'analysis': result,
            'timestamp': datetime.now().isoformat(),
            'analysis_type': 'comprehensive'
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/analyze/image', methods=['POST'])
def analyze_medical_image():
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø·Ø¨ÙŠØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…"""
    try:
        data = request.json
        image_data = data.get('image')
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø©
        image_bytes = base64.b64decode(image_data.split(',')[1])
        image = Image.open(io.BytesIO(image_bytes))
        image = image.resize((224, 224))
        image_array = np.array(image) / 255.0
        
        if len(image_array.shape) == 2:
            image_array = np.stack([image_array] * 3, axis=-1)
        
        image_array = np.expand_dims(image_array, axis=0)
        
        # Ù…Ø­Ø§ÙƒØ§Ø© ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ØµÙˆØ±Ø©
        result = {
            'image_quality': 'Ø¬ÙŠØ¯Ø©',
            'analysis_confidence': 0.85,
            'detected_features': ['Ø¹Ø¸Ø§Ù… Ø·Ø¨ÙŠØ¹ÙŠØ©', 'Ù„Ø§ ØªÙˆØ¬Ø¯ ÙƒØ³ÙˆØ± ÙˆØ§Ø¶Ø­Ø©'],
            'recommendations': 'Ø§Ù„ØµÙˆØ±Ø© ØªØ¸Ù‡Ø± Ø­Ø§Ù„Ø© Ø·Ø¨ÙŠØ¹ÙŠØ©ØŒ ÙˆÙ„ÙƒÙ† ÙŠÙÙ†ØµØ­ Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© Ø£Ø®ØµØ§Ø¦ÙŠ Ø§Ù„Ø£Ø´Ø¹Ø© Ù„Ù„ØªØ£ÙƒØ¯'
        }
        
        return jsonify({
            'success': True,
            'analysis': result,
            'timestamp': datetime.now().isoformat()
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/algorithms/info', methods=['GET'])
def get_algorithms_info():
    """Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙØµÙ„Ø© Ø¹Ù† Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª"""
    algorithms_info = {
        'logistic': {
            'name': 'Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ù„ÙˆØ¬Ø³ØªÙŠ',
            'type': 'ØªØµÙ†ÙŠÙÙŠ',
            'use_case': 'ØªØ´Ø®ÙŠØµ Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠØ© (Ù…ØµØ§Ø¨/ØºÙŠØ± Ù…ØµØ§Ø¨)',
            'accuracy': '85-90%',
            'speed': 'Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹'
        },
        'random_forest': {
            'name': 'Ø§Ù„ØºØ§Ø¨Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©',
            'type': 'ØªØµÙ†ÙŠÙÙŠ/ØªÙ†Ø¨Ø¤ÙŠ',
            'use_case': 'ØªØµÙ†ÙŠÙ Ù…Ø¹Ù‚Ø¯ Ù…Ø¹ Ø¹ÙˆØ§Ù…Ù„ Ù…ØªØ¹Ø¯Ø¯Ø©',
            'accuracy': '90-95%',
            'speed': 'Ø¹Ø§Ù„ÙŠØ©'
        },
        'svm': {
            'name': 'Ø¢Ù„Ø© Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø´Ø¹Ø§Ø¹ÙŠ',
            'type': 'ØªØµÙ†ÙŠÙÙŠ',
            'use_case': 'ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© ÙˆØ§Ù„ØµÙˆØ±',
            'accuracy': '85-92%',
            'speed': 'Ù…ØªÙˆØ³Ø·Ø©'
        },
        'knn': {
            'name': 'Ø£Ù‚Ø±Ø¨ Ø§Ù„Ø¬ÙŠØ±Ø§Ù†',
            'type': 'ØªØµÙ†ÙŠÙÙŠ',
            'use_case': 'Ø§Ù„ØªØ´Ø®ÙŠØµ Ø¨Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù…Ø¹ Ø­Ø§Ù„Ø§Øª Ø³Ø§Ø¨Ù‚Ø©',
            'accuracy': '80-85%',
            'speed': 'Ù…ØªÙˆØ³Ø·Ø©'
        },
        'naive_bayes': {
            'name': 'Ø¨Ø§ÙŠØ² Ø§Ù„Ø³Ø§Ø°Ø¬',
            'type': 'Ø§Ø­ØªÙ…Ø§Ù„ÙŠ',
            'use_case': 'Ø§Ù„ØªØ´Ø®ÙŠØµ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶',
            'accuracy': '75-85%',
            'speed': 'Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹'
        },
        'decision_tree': {
            'name': 'Ø´Ø¬Ø±Ø© Ø§Ù„Ù‚Ø±Ø§Ø±',
            'type': 'ØªØµÙ†ÙŠÙÙŠ',
            'use_case': 'Ù‚Ø±Ø§Ø±Ø§Øª Ø·Ø¨ÙŠØ© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙØ³ÙŠØ±',
            'accuracy': '80-88%',
            'speed': 'Ø¹Ø§Ù„ÙŠØ©'
        },
        'neural_network': {
            'name': 'Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©',
            'type': 'Ø´Ø§Ù…Ù„',
            'use_case': 'ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©',
            'accuracy': '90-95%',
            'speed': 'Ù…ØªÙˆØ³Ø·Ø©'
        },
        'cnn': {
            'name': 'Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ÙŠØ©',
            'type': 'Ø±Ø¤ÙŠØ© Ø­Ø§Ø³ÙˆØ¨ÙŠØ©',
            'use_case': 'ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø·Ø¨ÙŠØ© ÙˆØ§Ù„Ø£Ø´Ø¹Ø©',
            'accuracy': '92-98%',
            'speed': 'Ø¨Ø·ÙŠØ¦Ø©'
        },
        'lstm': {
            'name': 'Ø°Ø§ÙƒØ±Ø© Ù‚ØµÙŠØ±Ø© Ø·ÙˆÙŠÙ„Ø©',
            'type': 'ØªØ³Ù„Ø³Ù„ÙŠ',
            'use_case': 'Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ© ÙˆØªØ·ÙˆØ± Ø§Ù„Ø­Ø§Ù„Ø©',
            'accuracy': '88-93%',
            'speed': 'Ø¨Ø·ÙŠØ¦Ø©'
        },
        'transformer': {
            'name': 'Ø§Ù„Ù…Ø­ÙˆÙ„',
            'type': 'Ù…Ø¹Ø§Ù„Ø¬Ø© Ù„ØºÙˆÙŠØ©',
            'use_case': 'ÙÙ‡Ù… Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø·Ø¨ÙŠØ© Ø§Ù„Ù†ØµÙŠØ©',
            'accuracy': '90-96%',
            'speed': 'Ø¨Ø·ÙŠØ¦Ø©'
        }
    }
    
    return jsonify({
        'success': True,
        'algorithms': algorithms_info,
        'total_algorithms': len(algorithms_info),
        'recommendations': {
            'quick_diagnosis': ['logistic', 'naive_bayes'],
            'complex_analysis': ['random_forest', 'neural_network'],
            'image_analysis': ['cnn'],
            'time_series': ['lstm'],
            'text_analysis': ['transformer']
        }
    })

@app.route('/api/health', methods=['GET'])
def health_check():
    """ÙØ­Øµ ØµØ­Ø© Ø§Ù„Ø®Ø¯Ù…Ø©"""
    return jsonify({
        'status': 'healthy',
        'models_loaded': len(analyzer.models),
        'timestamp': datetime.now().isoformat(),
        'version': '2.0.0'
    })

if __name__ == '__main__':
    print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø®Ø¯Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø·Ø¨ÙŠ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©...")
    print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„Ø¹Ø´Ø±Ø©")
    print("ğŸŒ Ø§Ù„Ø®Ø¯Ù…Ø© Ù…ØªØ§Ø­Ø© Ø¹Ù„Ù‰: http://localhost:5000")
    app.run(host='0.0.0.0', port=5000, debug=True)
